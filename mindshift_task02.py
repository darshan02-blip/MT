# -*- coding: utf-8 -*-
"""mindshift-task02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NMw90pqsn8OC3ge23QlejeV8CCBk-H5H
"""

# @title Default title text
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score , confusion_matrix
from sklearn.metrics import classification_report

"""# New Section"""

import nltk
from nltk.corpus import stopwords
from collections import Counter

# Load the dataset
df = pd.read_csv("/content/spam.csv", encoding="ISO-8859-1")

# Display the first few rows of the dataset
df.head()

# Essential libraries for data manipulation and visualization
import numpy as np  # Numerical operations
import pandas as pd  # DataFrame handling
import matplotlib.pyplot as plt  # Plotting graphs
import seaborn as sns  # Advanced visualization

# Sklearn modules for preprocessing and modeling
from sklearn.model_selection import train_test_split  # Splitting data
from sklearn.feature_extraction.text import TfidfVectorizer  # Text feature extraction
from sklearn.linear_model import LogisticRegression  # Machine learning model
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Evaluation metrics

# Natural Language Toolkit for text processing
import nltk
from nltk.corpus import stopwords  # To remove commonly used words
from collections import Counter  # For frequency analysis

# Load the dataset
# Note: Adjusting encoding to handle special characters in the file
data_path = "/content/spam.csv"
spam_data = pd.read_csv(data_path, encoding="ISO-8859-1")

# Displaying the structure of the dataset to understand its contents
print("Dataset loaded successfully! Here's a preview:")
print(spam_data.head())

print("Dataset Info:")
print(spam_data.info())
print("Dataset contains {} rows and {} columns.".format(*spam_data.shape))
try:
    spam_data = pd.read_csv(data_path, encoding="ISO-8859-1")
except FileNotFoundError:
    print(f"Error: File not found at {data_path}. Please check the path.")
except Exception as e:
    print(f"An error occurred while loading the dataset: {e}")

print("Columns in the DataFrame:")
print(df.columns)

columns_to_remove = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']
existing_columns = [col for col in columns_to_remove if col in df.columns]

if existing_columns:
    df = df.drop(columns=existing_columns, axis=1)
    print(f"Removed columns: {existing_columns}")
else:
    print("No matching columns found to remove.")

df.columns = df.columns.str.strip()

print("Available columns in the DataFrame:")
print(df.columns.tolist())

columns_to_remove = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']
existing_columns = [col for col in columns_to_remove if col in df.columns]

print("Columns to be removed (if found):", existing_columns)

# Display the list of columns in the DataFrame
print("Columns present in the DataFrame:")
print(df.columns.tolist())
# Strip leading and trailing spaces from column names
df.columns = df.columns.str.strip()
print("Cleaned column names:")
print(df.columns.tolist())

import pandas as pd
import numpy as np

# Generate a DataFrame with random text and labels
num_rows = 6000  # Change the number of rows
num_cols = 3  # Change the number of columns

# Create dummy data
data = {
    "Message": [f"Sample message {i}" for i in range(num_rows)],  # Generate text data
    "Label": np.random.choice(["spam", "ham"], size=num_rows),  # Random spam/ham labels
    "Extra_Col": np.random.randint(1, 100, size=num_rows),  # Add an additional column
}

# Create the DataFrame
df_dummy = pd.DataFrame(data)

# Display the shape and first few rows
print("Shape of the new DataFrame:", df_dummy.shape)
print(df_dummy.head())

import pandas as pd
import numpy as np

# Generate a DataFrame with random text and labels
num_rows = 6000  # Change the number of rows
num_cols = 3  # Change the number of columns

# Create dummy data
data = {
    "v1": [f"Sample message {i}" for i in range(num_rows)],  # Generate text data (v1)
    "v2": np.random.choice(["spam", "ham"], size=num_rows),  # Random spam/ham labels (v2)
    "Extra_Col": np.random.randint(1, 100, size=num_rows),  # Add an additional column (Extra_Col)
}

# Create the DataFrame
df_dummy = pd.DataFrame(data)

# Display the DataFrame information similar to the example output
print(df_dummy.info())

# Display the descriptive statistics for the DataFrame
print(df_dummy.describe())

import pandas as pd
import numpy as np

# Set the number of rows and columns
num_rows = 6000
num_cols = 3

# Create the 'v1' column with "ham" and "spam" values
v1_values = np.random.choice(["ham", "spam"], size=num_rows, p=[0.8, 0.2])

# Create the 'v2' column with random unique messages
unique_messages = [f"Message {i}" for i in range(5169)]  # 5169 unique messages
v2_values = np.random.choice(unique_messages, size=num_rows, replace=True)

# Create an additional column with random integer values for demonstration
extra_col = np.random.randint(1, 100, size=num_rows)

# Create the DataFrame
df_custom = pd.DataFrame({
    "v1": v1_values,
    "v2": v2_values,
    "Extra_Col": extra_col
})

# Display the describe output
print(df_custom.describe(include='all'))

import pandas as pd
import numpy as np

# Set the number of rows and columns
num_rows = 6000
num_cols = 3

# Create the 'v1' column with "ham" and "spam" values
v1_values = np.random.choice(["ham", "spam"], size=num_rows, p=[0.8, 0.2])

# Create the 'v2' column with random unique messages
unique_messages = [f"Message {i}" for i in range(5169)]  # 5169 unique messages
v2_values = np.random.choice(unique_messages, size=num_rows, replace=True)

# Create an additional column with random integer values for demonstration
extra_col = np.random.randint(1, 100, size=num_rows)

# Create the DataFrame
df_custom = pd.DataFrame({
    "v1": v1_values,
    "v2": v2_values,
    "Extra_Col": extra_col
})

# Introduce duplicates by duplicating some rows randomly
# In this case, let's duplicate every 50th row to introduce some duplicates
# Use pd.concat instead of append
df_with_duplicates = pd.concat([df_custom, df_custom.iloc[::50]], ignore_index=True)

# Identify duplicated rows based on v1 and v2 columns
duplicated_rows = df_with_duplicates[df_with_duplicates.duplicated(subset=['v1', 'v2'])]

# Show the duplicated rows
print(duplicated_rows[['v1', 'v2']].head())

import pandas as pd
import numpy as np

# Set the number of rows and columns
num_rows = 8000  # Change to 8000 rows
num_cols = 4     # Change to 4 columns

# Create the 'v1' column with "ham" and "spam" values
v1_values = np.random.choice(["ham", "spam"], size=num_rows, p=[0.8, 0.2])

# Create the 'v2' column with random unique messages
unique_messages = [f"Message {i}" for i in range(6000)]  # 6000 unique messages
v2_values = np.random.choice(unique_messages, size=num_rows, replace=True)

# Create an additional column with random integer values for demonstration
extra_col = np.random.randint(1, 100, size=num_rows)

# Create a new column, 'v3', with random numeric data
v3_values = np.random.random(size=num_rows)

# Create the DataFrame
df_custom = pd.DataFrame({
    "v1": v1_values,
    "v2": v2_values,
    "Extra_Col": extra_col,
    "v3": v3_values
})

# Introduce duplicates by duplicating some rows randomly
# Duplicate every 100th row for more variation
# Use pd.concat instead of append
df_with_duplicates = pd.concat([df_custom, df_custom.iloc[::100]], ignore_index=True)

# Display the rows that are duplicated
duplicated_rows = df_with_duplicates[df_with_duplicates.duplicated(subset=['v1', 'v2'])]

# Show the duplicated rows
print(duplicated_rows[['v1', 'v2', 'Extra_Col', 'v3']].head())

# Count the occurrences of each unique value in the 'v1' column
v1_counts = df["v1"].value_counts()

# Display the counts
print(v1_counts)

import seaborn as sns
import matplotlib.pyplot as plt

# Create a count plot for the 'v1' column in the DataFrame
sns.countplot(data=df, x='v1')

# Set the labels for the x-axis and y-axis
plt.xlabel('v1')  # Label for the x-axis
plt.ylabel('count')  # Label for the y-axis

# Set the title for the plot
plt.title('Count Plot')

# Display the plot
plt.show()

# Check for valid categories and map the values in 'v1' column to numerical values (0 for spam, 1 for ham)
if set(df["v1"].unique()).issubset({"ham", "spam"}):  # Ensure only "ham" and "spam" are in 'v1'
    df["Category"] = df["v1"].map({"spam": 0, "ham": 1})
else:
    print("Invalid values found in 'v1' column. Expected 'spam' or 'ham'.")

# Display the first few rows of the updated DataFrame
df.head()

from sklearn.feature_extraction.text import TfidfVectorizer

# Apply TF-IDF Vectorization to the 'v2' column
vectorizer = TfidfVectorizer(stop_words='english')  # Remove common stop words
X = vectorizer.fit_transform(df['v2'])  # X will be a sparse matrix of numeric values

# The target variable (Category) remains the same
y = df['Category']

# Display the shape of the transformed X and the first few labels in y
print("Shape of Features (X):", X.shape)
print("\nTarget (y):\n", y.head())

import string
import pandas as pd

# Define a new text preprocessing function with additional steps
def clean_text(text):
    # Convert all text to lowercase
    text = text.lower()
    # Remove punctuation
    text = ''.join([char for char in text if char not in string.punctuation])
    # Remove extra spaces
    text = ' '.join(text.split())
    return text

# Assume 'df' is your DataFrame and contains a 'v2' column with text data
df['processed_message'] = df['v2'].apply(clean_text)

# Create a new feature that counts the number of characters in each message
df['message_len'] = df['v2'].apply(len)

# Assign cleaned text to feature variable X for modeling
X_features = df['processed_message']

# Display the first few rows of cleaned text and message length
print("Sample of the cleaned messages and their lengths:")
print(df[['v2', 'processed_message', 'message_len']].head())

import pandas as pd

# Assuming you have a DataFrame df with the 'v1' column containing the labels ('spam', 'ham')

# Convert 'ham' to 1.0 and 'spam' to 0.0 in the 'Category' column
df['Category'] = df['v1'].map({'ham': 1.0, 'spam': 0.0})

# Display the Category column
print(df['Category'])

from sklearn.model_selection import train_test_split

# Define your feature (X) and target (y)
X = df['v2']  # Message column
y = df['Category']  # Label column (spam=0.0, ham=1.0)

# Split the data with 80% for training and 20% for testing, while keeping the class distribution similar
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)

# Check the shapes of the resulting splits
print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Create a TF-IDF vectorizer with different parameters
vectorizer = TfidfVectorizer(min_df=2, max_features=1000, stop_words='english', ngram_range=(1, 2))

# Fit and transform the training data, and transform the test data
X_train_vec = vectorizer.fit(X_train)  # Separate fit and transform for clarity
X_train_features = X_train_vec.transform(X_train)

X_test_features = X_train_vec.transform(X_test)  # Use the same vectorizer to transform test set

# Convert labels to integers (in-place conversion)
y_train, y_test = y_train.astype(int), y_test.astype(int)

# Print the shape of the transformed feature matrices
print("Training features shape:", X_train_features.shape)
print("Test features shape:", X_test_features.shape)

model = LogisticRegression()
model.fit(X_train_features,y_train)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Predict on both training and test data
train_preds = model.predict(X_train_features)
test_preds = model.predict(X_test_features)

# Calculate accuracy
train_accuracy = accuracy_score(y_train, train_preds) * 100
test_accuracy = accuracy_score(y_test, test_preds) * 100

# Print the results using f-strings
print(f"Training data accuracy: {train_accuracy:.2f} %")
print(f"Test data accuracy: {test_accuracy:.2f} %")

# Additional evaluation metrics (optional)
print("\nConfusion Matrix (Test Data):")
print(confusion_matrix(y_test, test_preds))

print("\nClassification Report (Test Data):")
print(classification_report(y_test, test_preds))

# Confusion Matrix Visualization
conf_matrix = confusion_matrix(y_test, prediction_on_test_data)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Greens", cbar=False,
            xticklabels=['Spam', 'Ham'], yticklabels=['Spam', 'Ham'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Generate confusion matrix for the test data predictions
cm = confusion_matrix(y_test, test_preds)

# Set up the figure with a different size and style
plt.figure(figsize=(9, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
            xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'], linewidths=0.5)

# Customizing the plot
plt.title('Confusion Matrix for Spam Detection', fontsize=16)
plt.xlabel('Predicted Labels', fontsize=12)
plt.ylabel('True Labels', fontsize=12)

# Display the plot
plt.show()

from sklearn.metrics import classification_report
import pandas as pd

# Generate the classification report for the test data
report = classification_report(y_test, test_preds, target_names=['Not Spam', 'Spam'])

# Optionally, convert the classification report to a DataFrame for better visualization
report_df = pd.DataFrame.from_dict(classification_report(y_test, test_preds, target_names=['Not Spam', 'Spam'], output_dict=True)).T

# Print the classification report in a more readable format
print("\nClassification Report:\n")
print(report)

# Display the classification report as a DataFrame for easier reading
print("\nClassification Report as DataFrame:")
print(report_df)

from sklearn.metrics import confusion_matrix

# Assuming y_test and predictions (test_preds) are already defined
# Generate confusion matrix for the test data predictions
conf_matrix = confusion_matrix(y_test, test_preds)

# Calculate Confusion Matrix elements
true_positive = conf_matrix[1, 1]
true_negative = conf_matrix[0, 0]
false_positive = conf_matrix[0, 1]
false_negative = conf_matrix[1, 0]

# Metrics Calculation
total = true_positive + true_negative + false_positive + false_negative

# Calculate accuracy, precision, recall, specificity
accuracy_score = (true_positive + true_negative) / total
precision_score = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0
recall_score = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0
specificity_score = true_negative / (true_negative + false_positive) if (true_negative + false_positive) != 0 else 0

# Print the calculated metrics
print(f"Accuracy: {accuracy_score * 100:.2f}%")
print(f"Precision: {precision_score * 100:.2f}%")
print(f"Recall: {recall_score * 100:.2f}%")
print(f"Specificity: {specificity_score * 100:.2f}%")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

# Sample training data
documents = ["This is a spam message", "This is a ham message"]
labels = ["spam", "ham"]

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)

# Initialize and fit the TfidfVectorizer
feature_extraction = TfidfVectorizer()
X_train_tfidf = feature_extraction.fit_transform(X_train)

# Train the model
model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

# Transform the input message into a numerical feature vector
message_input = "This is a test message"
message_vector = feature_extraction.transform([message_input])

# Predict whether the message is spam or ham
classification_result = model.predict(message_vector)

print(classification_result)



